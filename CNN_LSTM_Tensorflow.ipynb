{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 16:27:29.361076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 16:27:30.044071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "2023-07-31 16:27:31.665995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 16:27:31.685853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-31 16:27:31.686074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "##Nice Work  kirtan\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from math import ceil, sqrt\n",
    "import natsort\n",
    "import glob\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# tf.config.set_visible_devices([], 'GPU')  # Hide GPU devices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# # Create some tensors and perform an operation\n",
    "# a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "# b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "# c = tf.matmul(a, b)\n",
    "\n",
    "# print(c)\n",
    "# tf.debugging.set_log_device_placement(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 42\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameDir_path = '/home/kirtan/Documents/Sign_Language/data/Frames/'\n",
    "checkpoint_path = '/home/kirtan/Documents/Sign_Language/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loud': 0, 'quiet': 1, 'happy': 2, 'sad': 3, 'beautiful': 4, 'ugly': 5, 'deaf': 6, 'blind': 7, 'nice': 8, 'mean': 9, 'rich': 10, 'poor': 11, 'thick': 12, 'thin': 13, 'expensive': 14, 'cheap': 15, 'flat': 16, 'curved': 17, 'male': 18, 'female': 19, 'tight': 20, 'loose': 21, 'high': 22, 'low': 23, 'soft': 24, 'hard': 25, 'deep': 26, 'shallow': 27, 'clean': 28, 'dirty': 29, 'strong': 30, 'weak': 31, 'dead': 32, 'alive': 33, 'heavy': 34, 'light': 35, 'famous': 36, 'long': 37, 'short': 38, 'tall': 39, 'wide': 40, 'narrow': 41, 'biglarge': 42, 'smalllittle': 43, 'slow': 44, 'fast': 45, 'hot': 46, 'cold': 47, 'warm': 48, 'cool': 49, 'new': 50, 'old': 51, 'young': 52, 'good': 53, 'bad': 54, 'wet': 55, 'dry': 56, 'sick': 57, 'healthy': 58}\n"
     ]
    }
   ],
   "source": [
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        json_file = json.load(f)\n",
    "    return json_file\n",
    "label_map = load_json('./label_map_temp_include.json')\n",
    "print(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = []\n",
    "filenames_train = []\n",
    "masks_train =[]\n",
    "labels_test = []\n",
    "filenames_test = []\n",
    "masks_test =[]\n",
    "labels_val = []\n",
    "filenames_val = []\n",
    "masks_val =[]\n",
    "seq_length=80\n",
    "def get_Lables(mode):\n",
    "    train_label = []\n",
    "    train_filename = []\n",
    "    train_mask=[]\n",
    "    train_split_file = f'./train_test_split/temp_include_{mode}.txt'\n",
    "    train_file = open(train_split_file, 'r')\n",
    "    for line in train_file:\n",
    "        label = \"\".join([i for i in line if i.isalpha()]).lower()\n",
    "        label = label[10:]\n",
    "        label = label[:-6]\n",
    "        line = line.split(\"/\")\n",
    "        line.pop(1)\n",
    "        line.insert(1,label)\n",
    "        last_word = line[-1].strip('\\n')\n",
    "        last_word = last_word + \"_frames\"\n",
    "        line[-1] = last_word\n",
    "        line = \"/\".join(line)\n",
    "        line = frameDir_path + line #frame folder\n",
    "        frame_names = os.listdir(line)\n",
    "        frame_names = natsort.natsorted(frame_names)\n",
    "        orginal_length = len(frame_names)\n",
    "        padding_length = seq_length - orginal_length\n",
    "        for i in range(orginal_length):\n",
    "            temp_fileName= os.path.join(line, frame_names[i])\n",
    "            train_filename.append(temp_fileName)\n",
    "        train_filename = train_filename + [\"\"] * padding_length\n",
    "    for frame in train_filename:\n",
    "        if frame != \"\":\n",
    "            split_frame = frame.split(\"/\")\n",
    "            train_label.append(split_frame[-3])\n",
    "            train_mask.append(True)  # Valid element, set mask to True\n",
    "        else:\n",
    "            train_label.append(-1)\n",
    "            train_mask.append(False)  # Padded element, set mask to False\n",
    "    for i in range(len(train_label)):\n",
    "        if train_label[i] != -1:\n",
    "            train_label[i] = label_map[train_label[i]]\n",
    "    train_file.close()\n",
    "    return np.array(train_filename),np.array(train_label),np.array(train_mask)\n",
    "\n",
    "filenames_train,labels_train,masks_train = get_Lables('train')\n",
    "filenames_val,labels_val,masks_val = get_Lables('val')\n",
    "# filenames_test,labels_test,masks_test = get_Lables('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_temp =[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "# print(len(list_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filenames_train.shape (46077,)\n",
      "labels_train.shape (46077,)\n",
      "masks_train.shape (46077,)\n",
      "filenames_val.shape (5299,)\n",
      "labels_val.shape (5299,)\n",
      "masks_val.shape (5299,)\n"
     ]
    }
   ],
   "source": [
    "print('filenames_train.shape',filenames_train.shape)\n",
    "print(\"labels_train.shape\",labels_train.shape)\n",
    "print(\"masks_train.shape\",masks_train.shape)\n",
    "# print(\"filenames_test.shape\",filenames_test.shape)\n",
    "# print(\"labels_test.shape\",labels_test.shape)\n",
    "# print(\"masks_test.shape\",masks_test.shape)\n",
    "print(\"filenames_val.shape\",filenames_val.shape)\n",
    "print(\"labels_val.shape\",labels_val.shape)\n",
    "print(\"masks_val.shape\",masks_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    if filename != \"\":\n",
    "        image = tf.io.read_file(filename)\n",
    "        print(image.shape)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "        print(image.shape)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        print(image.shape)\n",
    "        image = tf.image.resize(image, [64, 64])\n",
    "        print(image.shape)\n",
    "        image = tf.ensure_shape(image, (64, 64, 3))\n",
    "        print(image.shape)\n",
    "    else:\n",
    "        image = tf.zeros([64, 64,3], dtype=tf.float32)\n",
    "    label = tf.one_hot(label, 59)\n",
    "    label = tf.cast(label, tf.int64)  # Convert labels to int64\n",
    "    return image, label\n",
    "def train_preprocess(image, label):\n",
    "    if tf.reduce_all(image != 0):\n",
    "        image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image, 0.15)\n",
    "        image = tf.image.random_contrast(image, 0.8, 1.5)\n",
    "        image = tf.image.random_saturation(image, 0.6, 3)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(None, None, None)\n",
      "(None, None, None)\n",
      "(64, 64, None)\n",
      "(64, 64, 3)\n",
      "()\n",
      "(None, None, None)\n",
      "(None, None, None)\n",
      "(64, 64, None)\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 80\n",
    "padded_shapes = ([64, 64, 3],[59])\n",
    "# create train dataset\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((filenames_train, labels_train))\n",
    "dataset_train = dataset_train.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_train = dataset_train.map(train_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_train = dataset_train.batch(80,drop_remainder=True)\n",
    "dataset_train = dataset_train.batch(8,drop_remainder=True)\n",
    "\n",
    "# create validation dataset\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((filenames_val, labels_val))\n",
    "dataset_val = dataset_val.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_val = dataset_val.map(train_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_val = dataset_val.batch(80,drop_remainder=True)\n",
    "dataset_val = dataset_val.batch(8,drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 16:27:32.970046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [46077]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-31 16:27:32.970728: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [46077]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_len 8 (8, 80, 64, 64, 3)\n",
      "labels_len 8 (8, 80, 59)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(64, 64, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[39m#     print('batch',batch)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# print(len(batch))\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(dataset_train))\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(dataset_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in dataset_train:\n",
    "    images, labels = batch\n",
    "    print('image_len',len(images),images.shape)\n",
    "    print('labels_len',len(labels),labels.shape)\n",
    "    for image in images:\n",
    "        for img in image:\n",
    "            print(img.shape)\n",
    "#     print('batch',batch)\n",
    "    # print(len(batch))\n",
    "    break\n",
    "print(len(dataset_train))\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def testparse_function(filename,label):\n",
    "#     image = tf.io.read_file(filename)\n",
    "#     image = tf.image.decode_jpeg(image)\n",
    "#     image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#     image = tf.image.resize(image, [64, 64])\n",
    "#     image = tf.cast(image, tf.float32)\n",
    "#     label = tf.one_hot(label, 3)\n",
    "#     return image,label\n",
    "# def testtrain_preprocess(image,label):\n",
    "#     image = tf.image.random_brightness(image, 0.15)\n",
    "#     image = tf.image.random_contrast(image, 0.8, 1.5)\n",
    "#     image = tf.image.random_saturation(image, 0.6, 3)\n",
    "#     return image,label\n",
    "# padded_shapes = ([64, 64, 3],[3])\n",
    "# # create test dataset\n",
    "# dataset_test = tf.data.Dataset.from_tensor_slices((filenames_test,labels_test))\n",
    "# dataset_test = dataset_test.map(testparse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# # dataset_test = dataset_test.map(testtrain_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# dataset_test = dataset_test.padded_batch(batch_size, padded_shapes=padded_shapes,drop_remainder=True)\n",
    "# dataset_test = dataset_test.batch(3,drop_remainder=True)\n",
    "# dataset_test = dataset_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ReLU,TimeDistributed,Masking, Conv2D, ReLU, Dropout, Concatenate,MaxPooling2D, BatchNormalization, LSTM, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# class CRNN():\n",
    "#     def __init__(self, sample_size=64, sample_duration=80, num_classes=59,\n",
    "#                  lstm_hidden_size=512, lstm_num_layers=1):\n",
    "#         super(CRNN, self).__init__()\n",
    "#         print(x)\n",
    "#         self.sample_size = sample_size\n",
    "#         self.sample_duration = sample_duration\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         # network params\n",
    "#         self.ch1, self.ch2, self.ch3, self.ch4 = 64, 128, 256, 512\n",
    "#         self.k1, self.k2, self.k3, self.k4 = (2, 2), (2, 2), (2, 2), (2, 2)\n",
    "#         self.s1, self.s2, self.s3, self.s4 = (2, 2), (1, 1), (1, 1), (1, 1)\n",
    "#         self.p1, self.p2, self.p3, self.p4 = (0, 0), (0, 0), (0, 0), (0, 0)\n",
    "#         self.d1, self.d2, self.d3, self.d4 = (1, 1), (1, 1), (1, 1), (1, 1)\n",
    "#         self.lstm_input_size = self.ch4\n",
    "#         self.lstm_hidden_size = lstm_hidden_size\n",
    "#         self.lstm_num_layers = lstm_num_layers\n",
    "\n",
    "#         # network architecture\n",
    "#         self.conv1 = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch1, kernel_size=self.k1, strides=self.s1, padding='valid', dilation_rate=self.d1,input_shape=(None,80,3,64,64)),\n",
    "#             tf.keras.layers.BatchNormalization(momentum=0.01),\n",
    "#             tf.keras.layers.ReLU(),\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch1, kernel_size=1, strides=1),\n",
    "#             tf.keras.layers.MaxPool2D(pool_size=2,name='max_pooling2d_1'),\n",
    "#         ])\n",
    "#         self.conv2 = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch2, kernel_size=self.k2, strides=self.s2, padding='valid', dilation_rate=self.d2),\n",
    "#             tf.keras.layers.BatchNormalization(momentum=0.01),\n",
    "#             tf.keras.layers.ReLU(),\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch2, kernel_size=1, strides=1),\n",
    "#             tf.keras.layers.MaxPool2D(pool_size=2,name='max_pooling2d_2'),\n",
    "#         ])\n",
    "#         self.conv3 = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch3, kernel_size=self.k3, strides=self.s3, padding='valid', dilation_rate=self.d3),\n",
    "#             tf.keras.layers.BatchNormalization(momentum=0.01),\n",
    "#             tf.keras.layers.ReLU(),\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch3, kernel_size=1, strides=1),\n",
    "#             tf.keras.layers.MaxPool2D(pool_size=2,name='max_pooling2d_3'),\n",
    "#         ])\n",
    "#         self.conv4 = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch4, kernel_size=self.k4, strides=self.s4, padding='valid', dilation_rate=self.d4),\n",
    "#             tf.keras.layers.BatchNormalization(momentum=0.01),\n",
    "#             tf.keras.layers.ReLU(),\n",
    "#             tf.keras.layers.Conv2D(filters=self.ch4, kernel_size=1, strides=1),\n",
    "#             tf.keras.layers.GlobalAvgPool2D(name='gloabl_pooling2d_1'),\n",
    "#         ])\n",
    "#         self.lstm = tf.keras.layers.LSTM(units=self.lstm_hidden_size, return_sequences=True,stateful=True)\n",
    "#         self.fc1 = tf.keras.layers.Dense(units=self.num_classes)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         cnn_embed_seq = []\n",
    "#         print(x)\n",
    "#         for t in range(x.shape[2]):\n",
    "#             out = self.conv1(x[:, :, t, :, :])\n",
    "#             out = self.conv2(out)\n",
    "#             out = self.conv3(out)\n",
    "#             out = self.conv4(out)\n",
    "\n",
    "#             out = tf.reshape(out, (out.shape[0], -1))\n",
    "#             cnn_embed_seq.append(out)\n",
    "\n",
    "#         cnn_embed_seq = tf.stack(cnn_embed_seq, axis=0)\n",
    "#         cnn_embed_seq = tf.transpose(cnn_embed_seq, perm=[1, 0, 2])\n",
    "\n",
    "#         self.lstm.reset_states()\n",
    "#         out = self.lstm(cnn_embed_seq)\n",
    "#         out = self.fc1(out[:, -1, :])\n",
    "\n",
    "#         return out\n",
    "    \n",
    "\n",
    "def create_LRCN_model(input_shape=(80, 64, 64, 3), num_classes=59):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3),strides=(2,2), padding=\"valid\", activation='gelu',\n",
    "                                     use_bias=False, input_shape=input_shape,name=\"conv1\")))\n",
    "    model.add(TimeDistributed(BatchNormalization(momentum=0.01)))\n",
    "        \n",
    "    model.add(TimeDistributed(ReLU()))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (1, 1),strides=(1,1), activation='gelu',\n",
    "                                     use_bias=False,name=\"conv2\")))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    \n",
    "    ##Block 2\n",
    "    \n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3),strides=(1,1), padding=\"valid\", activation='gelu',\n",
    "                                     use_bias=False,name=\"conv3\")))\n",
    "    model.add(TimeDistributed(BatchNormalization(momentum=0.01)))\n",
    "\n",
    "    model.add(TimeDistributed(ReLU()))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(128, (1, 1),strides=(1,1), activation='gelu',\n",
    "                                     use_bias=False,name=\"conv4\")))\n",
    "        \n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))    \n",
    "    \n",
    "    ##Block 3\n",
    "    \n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(256, (3, 3),strides=(1,1),padding=\"valid\", activation='gelu',\n",
    "                                     use_bias=False,name=\"conv5\")))\n",
    "    model.add(TimeDistributed(BatchNormalization(momentum=0.01)))\n",
    "\n",
    "    model.add(TimeDistributed(ReLU()))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(256, (1, 1),strides=(1,1), activation='gelu',\n",
    "                                     use_bias=False,name=\"conv6\")))\n",
    "        \n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2)))) \n",
    "        \n",
    "        \n",
    "    \n",
    "    ##Block 4\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(256, (2, 2),strides=(1,1),padding=\"valid\", activation='gelu',\n",
    "                                     use_bias=False,name=\"conv7\")))\n",
    "    model.add(TimeDistributed(BatchNormalization(momentum=0.01)))\n",
    "\n",
    "    model.add(TimeDistributed(ReLU()))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(256, (1, 1),strides=(1,1), activation='gelu',\n",
    "                                     use_bias=False,name=\"conv8\")))\n",
    "        \n",
    "    model.add(TimeDistributed(GlobalAveragePooling2D())) \n",
    "    \n",
    "    ## LSTM \n",
    "    \n",
    "    model.add(TimeDistributed(Reshape((-1, 256))))  # Reshape to (time_steps, features)\n",
    "\n",
    "    model.add(TimeDistributed(LSTM(256, return_sequences=True)))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(256, activation=gelu, kernel_regularizer=tf.keras.regularizers.l2(1e-3))))\n",
    "\n",
    "    # Final dense layer for classification\n",
    "    model.add(TimeDistributed(Dense(num_classes,activation=gelu, kernel_regularizer=tf.keras.regularizers.l2(1e-3))))\n",
    "\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRCN_model = create_LRCN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 16:31:31.256391: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-08 16:31:31.259086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-08 16:31:31.261147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-08 16:31:31.841264: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-08 16:31:31.843594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-08 16:31:31.845216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (8, 80, 59) and (8, 80, 1, 59) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m cp_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_path,\n\u001b[1;32m      5\u001b[0m                                                   save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      6\u001b[0m                                                   save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m LRCN_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcategorical_crossentropy,\n\u001b[1;32m      8\u001b[0m                    optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m      9\u001b[0m                    metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                             tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(),\n\u001b[1;32m     13\u001b[0m                             ])\n\u001b[0;32m---> 14\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mLRCN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crossway/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileiag_3q9v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/kirtan/.conda/envs/crossway/lib/python3.9/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (8, 80, 59) and (8, 80, 1, 59) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# tf.debugging.set_log_device_placement(False)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Set TensorFlow logging level to suppress messages\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                  save_weights_only=True, monitor='val_loss', verbose=1, \n",
    "                                                  save_best_only=True, mode='min')\n",
    "LRCN_model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
    "                            tf.keras.metrics.RecallAtPrecision(precision=0.9, name='recallAtPrecision'),\n",
    "                            tf.keras.metrics.Recall(),\n",
    "                            tf.keras.metrics.Precision(),\n",
    "                            ])\n",
    "hist = LRCN_model.fit(x = dataset_train,validation_data=dataset_val,verbose=1, epochs = 50,callbacks = [cp_callback,early_stopping_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80, 64, 64, 3)\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "(1, 80, 59)\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/kirtan/Documents/Sign_Language/data/Frames/Adjectives/dry/MVI_5167.MOV_frames\"\n",
    "frame_pathList = []\n",
    "image_sequence=[]\n",
    "for i in range(40):\n",
    "    temp_path = folder_path + f'/frame_{i}.jpg'\n",
    "    frame_pathList.append(temp_path)\n",
    "    temp_path = \"\"\n",
    "\n",
    "for path in frame_pathList:\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    image_sequence.append(image)\n",
    "desired_length = 80\n",
    "padding_length = desired_length - len(image_sequence)\n",
    "\n",
    "if padding_length > 0:\n",
    "    padding_images = tf.zeros([padding_length, 64, 64, 3], dtype=tf.float32)\n",
    "    image_sequence = tf.concat([image_sequence, padding_images], axis=0)\n",
    "\n",
    "image_sequence = tf.stack(image_sequence, axis=0)  # Create the image sequence\n",
    "image_sequence = tf.expand_dims(image_sequence, axis=0)\n",
    "print(image_sequence.shape) \n",
    "predictions = LRCN_model.predict(image_sequence)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = LRCN_model.evaluate(dataset_test)\n",
    "# print(\"test loss, test acc:\", results)\n",
    "\n",
    "print(\"Evaluate on train data\")\n",
    "results = LRCN_model.evaluate(dataset_train)\n",
    "print(\"train loss, trai acc:\", results)\n",
    "\n",
    "print(\"Evaluate on validation data\")\n",
    "results = LRCN_model.evaluate(dataset_val)\n",
    "print(\"val loss, val acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.0246892  0.03619457 0.04104808 ... 0.05684435 0.02114256 0.04000509]\n",
      "  [0.02373577 0.03230406 0.04580003 ... 0.05955273 0.0194433  0.04120204]\n",
      "  [0.02351937 0.03051795 0.04802994 ... 0.06088576 0.01844789 0.04187407]\n",
      "  ...\n",
      "  [0.01146932 0.01168903 0.02472704 ... 0.00239041 0.01938326 0.00646256]\n",
      "  [0.01106928 0.01096132 0.02407236 ... 0.00230767 0.01911121 0.0065612 ]\n",
      "  [0.01011055 0.01008899 0.0239671  ... 0.00205243 0.0185706  0.00601373]]]\n",
      "[[45 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53\n",
      "  53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53  2  2 57 57 57 44 44 44\n",
      "  44 44 44 44 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 34 20 20\n",
      "  20 20 20 20 20 20 20 20]]\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "predicted_labels = np.argmax(predictions, axis=2)\n",
    "print(predicted_labels)\n",
    "final = np.max(predicted_labels)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 24, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "# With `dilation_rate` as 2.\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "    2, 3,\n",
    "    activation='relu',\n",
    "    dilation_rate=2)(x)\n",
    "print(y.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
